{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m0k08nh/projects/preparing/test/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers einops accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "import base64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "def slm_pipeline():\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 256,\n",
    "        do_sample = True,\n",
    "        temperature = 0.3,\n",
    "        top_p = 0.95\n",
    "    )\n",
    "    local_slm = HuggingFacePipeline(pipeline = pipe)\n",
    "    return local_slm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Write an article about Blockchain and its benefits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "model = slm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/d5kbr2ws10x8yxm5dk18ts_40000gp/T/ipykernel_5987/3568923136.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  gen_text = model(input_prompt)\n"
     ]
    }
   ],
   "source": [
    "gen_text = model(input_prompt)\n",
    "#gen_text = model.invoke(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blockchain is a decentralized digital ledger that records transactions in a secure and transparent manner. It is a decentralized system that allows for secure and transparent transactions without the need for intermediaries like banks or governments. Blockchain has numerous benefits, including: 1. Security: Blockchain technology provides a high level of security as it is designed to protect sensitive information from unauthorized access. This makes it ideal for applications such as financial transactions, supply chain management, and voting systems. 2. Transparency: Blockchain is transparent, which means that it is not influenced by any one person or entity. This means that it is impossible to'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.24\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/m0k08nh/projects/preparing/test/venv/lib/python3.12/site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: langchain-community, langchain-neo4j, langchain-tavily\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers einops accelerate bitsandbytes\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import base64\n",
    "\n",
    "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "!pip install langchain langchain-community langchain-huggingface\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "def slm_pipeline():\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 256,\n",
    "        do_sample = True,\n",
    "        temperature = 0.3,\n",
    "        top_p = 0.95\n",
    "    )\n",
    "    local_slm = HuggingFacePipeline(pipeline = pipe)\n",
    "    return local_slm\n",
    "    \n",
    "\n",
    "\n",
    "input_prompt = \"Write an article about Blockchain and its benefits\"\n",
    "\n",
    "model = slm_pipeline()\n",
    "gen_text = model.invoke(input_prompt)\n",
    "gen_text\n",
    "\n",
    "\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'MBZUAI/LaMini-T5-738M',\n",
    "\t'HF_TASK' : 'text2text-generation',\n",
    "    'device_map' : 'auto',\n",
    "    'torch_dtype' : 'torch.float32'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"3.2.3\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Write an article about Cyber Security\",\n",
    "})\n",
    "\n",
    "ENDOINT = \"your endpoint\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name='us-east-1')\n",
    "\n",
    "endpoint_name = ENDPOINT\n",
    "\n",
    "# API Payload\n",
    "prompt = \"Write an article on Deep learning\"\n",
    "\n",
    "payload = {\n",
    "    'inputs' : prompt ,\n",
    "    'parameters' : {\n",
    "        'max_new_tokens' : 256,\n",
    "        'do_sample' : True,\n",
    "        'temperature' : 0.3,\n",
    "        'top_p' : 0.7,\n",
    "        'top_k' : 50,\n",
    "        'repetion_penalty' : 1.03\n",
    "    }\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    ContentType = \"application/json\",\n",
    "    Body = json.dumps(payload)\n",
    ")\n",
    "\n",
    "predictions = json.loads(response['Body'].read().decode('utf-8'))\n",
    "final_result =predictions[0]['generated_text']\n",
    "\n",
    "\n",
    "##############LAMBDA###############\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "ENDPOINT = \"80\"\n",
    "\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name='us-east-1')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    query_params = event['queryStringParameters']\n",
    "    query = query_params['query']\n",
    "    payload = {\n",
    "        \"inputs\": query,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_k\": 50,\n",
    "            \"repetion_penalty\" : 1.03,\n",
    "            \"do_sample\" : True\n",
    "        }\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(EndpointName=ENDPOINT,\n",
    "                                                  ContentType=\"application/json\",\n",
    "                                                  Body=json.dumps(payload))\n",
    "    predictions = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    final_result = predictions[0]['generated_text']\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(final_result)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
